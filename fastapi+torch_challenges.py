# -*- coding: utf-8 -*-
"""FastAPI+Torch_challenges.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kpUmTRHC7ZhUthe6sJgadZm1J5V96_0
"""

#instalaciones

#!pip3 install torchrec-nightly

!pip install fastapi
!pip install streamlit
!npm install -g localtunnel
!pip install pyngrok
!pip install uvicorn[standard]
!pip install ssl
!pip install pandas

import uvicorn
from typing import Union, Dict
from fastapi import FastAPI, Response, Depends


#from fastapi.encoders import jsonable_encoder
#from fastapi.responses import JSONResponse
from pydantic import BaseModel
import asyncio

import IPython
#from IPython.display import data
import google.colab
from google.colab import output
#import pandas

from IPython.display import Javascript
display(Javascript('''
(async () => {
  google.colab.kernel.comms.registerTarget('comms_testing', (comm, message) => {
    comm.send('this is the response', {buffers: message.buffers});
    document.body.appendChild(document.createTextNode('comm opened.'))
  });
})()'''))

from ipykernel import comm
buffer = b'hello world'
channel = comm.Comm(target_name='comms_testing', data={'foo': 1}, buffers=[buffer])

message = None
def handle_message(msg):
  global message
  message = msg

channel.on_msg(handle_message)

!wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh
!chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh
!bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local

!conda install pytorch pytorch-cuda=11.8 -c pytorch-nightly -c nvidia -y

#from google.colab import drive
#drive.mount('/content/drive')

!cp /usr/local/lib/lib* /usr/lib/

#importaciones
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import csv
import ast
from torch.utils.data import Dataset, DataLoader
import requests
import pprint
import sys
sys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages']

import portpicker
import threading
import socket
import IPython

from six.moves import socketserver
from six.moves import SimpleHTTPServer

#carga del modelo de Torch
class UserItemDataset(Dataset):
    def __init__(self, user_data, item_data, ratings):
        self.user_data = torch.LongTensor(user_data)
        self.item_data = torch.LongTensor(item_data)
        self.ratings = torch.FloatTensor(ratings) #cambiar esto por varios parametros

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        user = self.user_data[idx]
        item = self.item_data[idx]
        rating = self.ratings[idx] #Parámetros deseados: %desafíos realizados en la habilidad correspondiente
        return user, item, rating

# definimos la clase para recomendar y para ser recomendado
class recommender(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim): #van 3 parámetros
        super(recommender, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        self.fc = nn.Linear(embedding_dim * 2, 1)

    def forward(self, user, item):
        user_embed = self.user_embedding(user)
        item_embed = self.item_embedding(item)
        concat_embed = torch.cat([user_embed, item_embed], dim=1)
        output = self.fc(concat_embed)
        return output.squeeze()

#cargar base de datos (!pendiente, conexión de verdad)
user_data=[] # ID de grupo
ratings = [] # Respuestas habilidades

with open('boldinnGroups.csv') as csv_file:
  csv_reader = csv.reader(csv_file, delimiter=',')
  line_count = 0
  for row in csv_reader:
        if line_count == 0:
            print(f'Column names are {", ".join(row)}')
            line_count += 1
        else:
            float1 = float(row[0])
            int1 = round(float1)
            user_data.append(int1)
            data2_dict = ast.literal_eval(row[2])
            ratings = list(data2_dict.values())
            rounded_ratings = [round(value, 2) for value in ratings]
            line_count += 1
print(f'Processed {line_count} lines.')
#for i in ratings: ratings[i] = round(ratings[i])   !pendiente redondear!

print(f'\t{user_data} has the criteria {rounded_ratings}.')


item_data=[]
# task IDs (usar "code")
with open('boldinnTasks.csv') as csv_file:
  csv_reader = csv.reader(csv_file, delimiter=',')
  line_count = 0
  for row in csv_reader:
    if line_count == 0:
            print(f'Column names are {", ".join(row)}')
            line_count += 1
    else:
            print(f'\t{row[0]} has the criteria {row[3]}')
            item_data.append(round(float(row[3])))
            line_count += 1
  print(f'Processed {line_count} lines.')



#carga de los datos en una lista
dataset = UserItemDataset(user_data, item_data, rounded_ratings)
dataloader = DataLoader(dataset, batch_size=10, shuffle=False)

#entrenar recomendador (! PENDIENTE: redondear parametros de entrada, revisar criterio de optimización)
num_users = len(user_data)
num_items = len(item_data)
embedding_dim = 128
#selección del modelo
model = recommender(num_users, num_items, embedding_dim)
#configuracion del criterio y su optimizador
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.5)

print(model)
#print(model.plan)

#solicitud para recomendacion (! PENDIENTE conectar con el Ruby)
recommendations = []
test_user = torch.LongTensor([0])  # Ingresamos "groupID" (indexado)

for i in item_data:
 test_item = torch.LongTensor([i])  # Ingresamos "code" (indexado)
 #iteraciones del modelo recomendador
 with torch.no_grad():
    recommendations.append(model(test_user, test_item))

#salida de depuración
matrix = [recommendations[i:i+8] for i in range(0, len(recommendations), 8)]
#matrix_df = pd.DataFrame(matrix)
#print(recommendations)
display(matrix)

port = 8000
url = ""
host = ""

#Configuración de API
app = FastAPI()

@app.route("/")
async def home():
    return "<h1>test</h1>"

@app.get("/")
async def read_root():
    return {"Hello": "World"}

@app.get("/getrec")
async def getrec():
   return IPython.display.JSON({'result': ' '.join((recommendations))})

try:
  print(recommendations)

except Exception as err:
  print(str(err))

if __name__ == "":
    app.run(debug=True, host=host, port=int(os.environ.get("PORT", 8000)))

import requests
from tenacity import (retry, stop_after_attempt, wait_fixed,
                      retry_if_exception_type)

@retry(stop=stop_after_attempt(3), wait=wait_fixed(0.1),
      retry=retry_if_exception_type(requests.HTTPError))
def get(url):
    try:
        r = requests.get(url)
        r.raise_for_status()  # raise an error on a bad status
        return r
    except requests.HTTPError:
        print(r.status_code, r.reason)
        raise

api_key = 'clavealeatoriadealtacalidad'
r = requests.get(url, auth=(api_key, ''))